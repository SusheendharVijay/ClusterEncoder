{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liberal-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "from collections import OrderedDict\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "weird-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm \n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from torch import nn,optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "living-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self,layers,clusters):\n",
    "        super(GraphEncoder,self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(OrderedDict({\n",
    "            \"lin1\": nn.Linear(layers[0],layers[1]),\n",
    "            \"sig1\": nn.Sigmoid(),\n",
    "            \"lin2\": nn.Linear(layers[1],layers[2]),\n",
    "            \"sig2\": nn.Sigmoid(),\n",
    "            \"lin3\": nn.Linear(layers[2],layers[3]),\n",
    "            \"sig3\": nn.Sigmoid(),\n",
    "            \"lin4\": nn.Linear(layers[3],layers[4]),\n",
    "            \"sig4\": nn.Sigmoid(),\n",
    "        }))\n",
    "        \n",
    "        \n",
    "        self.clusters = clusters\n",
    "        self.outputs = {}\n",
    "        \n",
    "        self.layers[0].register_forward_hook(self.get_activation(\"lin1\"))\n",
    "        self.layers[2].register_forward_hook(self.get_activation(\"lin2\"))\n",
    "        self.layers[4].register_forward_hook(self.get_activation(\"lin3\"))\n",
    "        \n",
    "    def get_activation(self,name):\n",
    "        def hook(module,input,output):\n",
    "            self.outputs[name] = output\n",
    "        return hook\n",
    "    \n",
    "    def forward(self,X):\n",
    "        output = self.layers(X)\n",
    "        return output\n",
    "    \n",
    "    def layer_activations(self,layername):\n",
    "       # print(torch.sigmoid(self.outputs[layername]).shape)\n",
    "        return torch.mean(torch.sigmoid(self.outputs[layername]),dim=0)\n",
    "    \n",
    "    def sparse_result(self,rho,layername):\n",
    "        rho_hat = self.layer_activations(layername)\n",
    "        return rho * np.log(rho) - rho * torch.log(rho_hat) + (1 - rho) * np.log(1 - rho) \\\n",
    "                - (1 - rho) * torch.log(1 - rho_hat)\n",
    "    \n",
    "    def kl_div(self,rho):\n",
    "        first = torch.mean(self.sparse_result(rho,\"lin1\"))\n",
    "        second = torch.mean(self.sparse_result(rho,\"lin2\"))\n",
    "        return first + second\n",
    "    \n",
    "    def get_index_by_name(self,name):\n",
    "        return list(dict(self.layers.named_children()).keys()).index(name)\n",
    "    \n",
    "    def loss(self,x_hat,x,beta,rho):\n",
    "        loss = F.mse_loss(x_hat,x) + beta*self.kl_div(rho)\n",
    "        return loss \n",
    "    \n",
    "    def get_cluster(self):\n",
    "        kmeans = KMeans(n_clusters = self.clusters).fit(self.outputs[\"lin2\"].detach().cpu().numpy())\n",
    "        self.centroids = kmeans.cluster_centers_\n",
    "        return kmeans.labels_\n",
    "    \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "entertaining-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self,input_layer,hidden_layer):\n",
    "        super(SAE,self).__init__()\n",
    "        self.layers = [input_layer] + [hidden_layer] + [input_layer]\n",
    "#         print(self.layers)\n",
    "        self.net = nn.Sequential(OrderedDict({\n",
    "            \"lin1\": nn.Linear(self.layers[0],self.layers[1]),\n",
    "            \"sig1\":nn.Sigmoid(),\n",
    "            \"lin2\": nn.Linear(self.layers[1],self.layers[2]),\n",
    "            \"sig2\":nn.Sigmoid()\n",
    "        }))\n",
    "        \n",
    "        self.outputs = {}\n",
    "        \n",
    "        self.net[0].register_forward_hook(self.get_activation(\"lin1\"))\n",
    "        \n",
    "        \n",
    "    def get_activation(self,name):\n",
    "        def hook(module,input,output):\n",
    "            self.outputs[name] = output\n",
    "        return hook\n",
    "\n",
    "    def forward(self,X):\n",
    "        output = self.net(X)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def layer_activations(self,layername):\n",
    "        return torch.mean(torch.sigmoid(self.outputs[layername]),dim=0)\n",
    "\n",
    "    def sparse_result(self,rho,layername):\n",
    "        rho_hat = self.layer_activations(layername)\n",
    "        return rho * np.log(rho) - rho * torch.log(rho_hat) + (1 - rho) * np.log(1 - rho) \\\n",
    "            - (1 - rho) * torch.log(1 - rho_hat)\n",
    "\n",
    "    def kl_div(self,rho):\n",
    "        return torch.mean(self.sparse_result(rho,\"lin1\"))\n",
    "\n",
    "    def loss(self,x_hat,x,rho,beta):\n",
    "        loss = F.mse_loss(x_hat,x) + beta*self.kl_div(rho)\n",
    "        return loss\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "administrative-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset\": \"wine\",\n",
    "    \"layers\":[128,64,128],\n",
    "    \"beta\": 0.01,\n",
    "    \"rho\":0.5,\n",
    "    \"lr\": 0.01,\n",
    "    \"epoch\": 200,\n",
    "    \"device\":\"gpu\"\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if args[\"device\"] == \"gpu\" else \"cpu\")\n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-tomorrow",
   "metadata": {},
   "source": [
    "## Stacked Auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "unusual-salad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 31.89it/s, loss=tensor(3.1646e-05, device='cuda:0', grad_fn=<AddBackward0>), nmi=0.628]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 2 0 0 0 2 0 2 0 2 0\n",
      " 0 2 0 2 0 2 0 0 2 1 0 0 2 2 2 2 2 2 2 2 0 0 2 0 0 2 0 2 2 2 2 2 2 2 2 0 0\n",
      " 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if args[\"dataset\"].lower() == \"wine\":\n",
    "        data = load_wine()\n",
    "    else:\n",
    "        raise Exception(\"Invalid Dataset\")\n",
    "    \n",
    "    X  = data.data\n",
    "    y = data.target\n",
    "    k = len(np.unique(y))\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X = min_max_scaler.fit_transform(X)\n",
    "    \n",
    "    S = cosine_similarity(X,X)\n",
    "    \n",
    "    D  = np.diag(1.0/S.sum(axis=1))\n",
    "    \n",
    "    X_train = torch.tensor(D.dot(S)).float().to(device)\n",
    "    \n",
    "    layers = [len(X_train)] + args[\"layers\"] + [len(X_train)]\n",
    "    \n",
    "    model = GraphEncoder(layers,k).to(device)\n",
    "    optimizer  = optim.Adam(model.parameters(),lr=args[\"lr\"])\n",
    "    \n",
    "    with tqdm(total= args[\"epoch\"]) as tq:\n",
    "        for epoch in range(1,args[\"epoch\"] +1):\n",
    "            optimizer.zero_grad()\n",
    "            X_hat = model(X_train)\n",
    "            loss = model.loss(X_hat,X_train, args[\"beta\"],args[\"rho\"])\n",
    "            nmi = normalized_mutual_info_score(model.get_cluster(),y,average_method=\"arithmetic\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tq.set_postfix(loss=loss,nmi =\"{:.3f}\".format(nmi))\n",
    "            tq.update()\n",
    "        print(model.get_cluster())\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "shared-newman",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 332.49it/s, loss=tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      " 32%|███▎      | 65/200 [00:00<00:00, 335.21it/s, loss=tensor(1.3309, device='cuda:0', grad_fn=<AddBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 336.98it/s, loss=tensor(1.3227, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      " 30%|██▉       | 59/200 [00:00<00:00, 342.69it/s, loss=tensor(0.1344, device='cuda:0', grad_fn=<AddBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 331.88it/s, loss=tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 32])\n",
      "0.6278680701415912\n"
     ]
    }
   ],
   "source": [
    "hyper = {\n",
    "    \"beta\": 0.05,\n",
    "    \"rho\":0.2,\n",
    "    \"lr\": 0.01,\n",
    "    \"epoch\": 200,\n",
    "\n",
    "}\n",
    "\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "k = len(np.unique(y))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "S = cosine_similarity(X,X)\n",
    "D  = np.diag(1.0/S.sum(axis=1))\n",
    "X_train = torch.tensor(D.dot(S)).float().to(device)\n",
    "\n",
    "layers = [128,64,32]\n",
    "inp_layers = [178,128,64]\n",
    "\n",
    "\n",
    "for inp_layer,hidden_layer in zip(inp_layers,layers):\n",
    "    model = SAE(inp_layer,hidden_layer).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=hyper[\"lr\"])\n",
    "    with tqdm(total=hyper[\"epoch\"]) as tq:\n",
    "        for epoch in range(1,hyper[\"epoch\"]+1):\n",
    "            optimizer.zero_grad()\n",
    "            x_hat = model(X_train)\n",
    "            loss = model.loss(x_hat,X_train,hyper[\"rho\"],hyper[\"beta\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tq.set_postfix(loss=loss)\n",
    "            tq.update()\n",
    "    print(model.outputs[\"lin1\"].shape)\n",
    "    X_train = model.outputs[\"lin1\"].detach()\n",
    "    \n",
    "    \n",
    "           \n",
    "kmeans = KMeans(n_clusters=k).fit(X_train.detach().cpu().numpy())\n",
    "labels = kmeans.labels_\n",
    "nmi = normalized_mutual_info_score(labels,y,average_method=\"arithmetic\")\n",
    "print(nmi)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "liked-sympathy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=3)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "k = len(np.unique(y))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "S = cosine_similarity(X,X)\n",
    "D  = np.diag(1.0/S.sum(axis=1))\n",
    "X_train = torch.tensor(D.dot(S)).float().numpy()\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "faced-bridges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6351524906645798\n"
     ]
    }
   ],
   "source": [
    "nmi = normalized_mutual_info_score(kmeans.labels_,y,average_method=\"arithmetic\")\n",
    "print(nmi)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-percentage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
